{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Utility Functions & Helpers\n",
    "## Common utilities for all modules\n",
    "\n",
    "**Purpose**: Shared utility functions for text processing, data handling, and file I/O\n",
    "\n",
    "**Can run independently?** ✅ YES (2 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numpy pandas scikit-learn nltk -q\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove special characters but keep common punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,-]', '', text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\ndef normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whitespace in text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "    \n",
    "    Returns:\n",
    "        Text with normalized whitespace\n",
    "    \"\"\"\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\ndef extract_keywords(text: str, num_keywords: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract keywords from text (simple TF-IDF approach).\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        num_keywords: Number of keywords to extract\n",
    "    \n",
    "    Returns:\n",
    "        List of keywords\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    # Simple keywords from text\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Filter common stop words\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'was', 'are', 'be', 'to', 'of', 'in', 'at', 'for'}\n",
    "    keywords = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    \n",
    "    # Get unique and most frequent\n",
    "    from collections import Counter\n",
    "    keyword_counts = Counter(keywords)\n",
    "    \n",
    "    return [word for word, _ in keyword_counts.most_common(num_keywords)]\n",
    "\n",
    "\ndef remove_duplicates(text_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove duplicate strings while preserving order.\n",
    "    \n",
    "    Args:\n",
    "        text_list: List of texts\n",
    "    \n",
    "    Returns:\n",
    "        List with duplicates removed\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        if text not in seen:\n",
    "            seen.add(text)\n",
    "            result.append(text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\ndef split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "    \n",
    "    Returns:\n",
    "        List of sentences\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "print(\"✓ Text processing utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequences(sequences: List[Any], batch_size: int):\n",
    "    \"\"\"\n",
    "    Create batches from sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequences\n",
    "        batch_size: Size of each batch\n",
    "    \n",
    "    Yields:\n",
    "        Batches of sequences\n",
    "    \"\"\"\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        yield sequences[i:i + batch_size]\n",
    "\n",
    "\ndef pad_sequences(sequences: List[List[int]], max_len: int, pad_value: int = 0) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Pad sequences to maximum length.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequences\n",
    "        max_len: Maximum length\n",
    "        pad_value: Value to use for padding\n",
    "    \n",
    "    Returns:\n",
    "        Padded sequences\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [pad_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded.append(seq)\n",
    "    \n",
    "    return padded\n",
    "\n",
    "\ndef split_data(data: List[Any], train_ratio: float = 0.8, shuffle: bool = True) -> Tuple[List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Split data into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        data: Data to split\n",
    "        train_ratio: Ratio for training set\n",
    "        shuffle: Whether to shuffle before split\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_data, test_data)\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        import random\n",
    "        data = random.sample(data, len(data))\n",
    "    \n",
    "    split_point = int(len(data) * train_ratio)\n",
    "    \n",
    "    return data[:split_point], data[split_point:]\n",
    "\n",
    "\ndef create_dataframe(data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create pandas DataFrame from list of dicts.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"✓ Data processing utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File I/O Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Load JSON file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\ndef save_json(data: Dict, file_path: str, pretty: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Save data to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: Data to save\n",
    "        file_path: Path to save to\n",
    "        pretty: Whether to pretty-print\n",
    "    \"\"\"\n",
    "    Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        if pretty:\n",
    "            json.dump(data, f, indent=2)\n",
    "        else:\n",
    "            json.dump(data, f)\n",
    "\n",
    "\ndef load_pickle(file_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to pickle file\n",
    "    \n",
    "    Returns:\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\ndef save_pickle(data: Any, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save data to pickle file.\n",
    "    \n",
    "    Args:\n",
    "        data: Data to save\n",
    "        file_path: Path to save to\n",
    "    \"\"\"\n",
    "    Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\ndef load_text_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load text file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to text file\n",
    "    \n",
    "    Returns:\n",
    "        File contents\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\ndef save_text_file(text: str, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save text to file.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to save\n",
    "        file_path: Path to save to\n",
    "    \"\"\"\n",
    "    Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"✓ File I/O utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate similarity between two texts (0-1).\n",
    "    \n",
    "    Args:\n",
    "        text1: First text\n",
    "        text2: Second text\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score\n",
    "    \"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "    \n",
    "    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "\ndef calculate_levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein distance between two strings.\n",
    "    \n",
    "    Args:\n",
    "        s1: First string\n",
    "        s2: Second string\n",
    "    \n",
    "    Returns:\n",
    "        Distance (0 = identical)\n",
    "    \"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return calculate_levenshtein_distance(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "\ndef calculate_jaccard_similarity(set1: set, set2: set) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between two sets.\n",
    "    \n",
    "    Args:\n",
    "        set1: First set\n",
    "        set2: Second set\n",
    "    \n",
    "    Returns:\n",
    "        Jaccard similarity (0-1)\n",
    "    \"\"\"\n",
    "    if len(set1 | set2) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "print(\"✓ Text similarity utilities created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Text cleaning\ntest_text = \"  Hello  WORLD  !  \"\ncleaned = clean_text(test_text)\nassert \"hello world\" in cleaned.lower()\nprint(f\"✓ clean_text: '{test_text}' → '{cleaned}'\")\n\n# Test 2: Keyword extraction\ntest_doc = \"Python is a great programming language. Python is powerful.\"\nkeywords = extract_keywords(test_doc, num_keywords=5)\nassert len(keywords) > 0\nprint(f\"✓ extract_keywords: {keywords}\")\n\n# Test 3: Sentence splitting\ntest_sentence = \"Hello world. How are you? I am fine.\"\nsentences = split_into_sentences(test_sentence)\nassert len(sentences) == 3\nprint(f\"✓ split_into_sentences: {len(sentences)} sentences\")\n\n# Test 4: Batching\ndata = list(range(10))\nbatches = list(batch_sequences(data, 3))\nassert len(batches) == 4\nprint(f\"✓ batch_sequences: {len(batches)} batches from 10 items\")\n\n# Test 5: Padding\nseqs = [[1, 2, 3], [1, 2]]\npadded = pad_sequences(seqs, max_len=5, pad_value=0)\nassert all(len(s) == 5 for s in padded)\nprint(f\"✓ pad_sequences: Padded to length 5\")\n\n# Test 6: Train/Test split\ndata = list(range(100))\ntrain, test = split_data(data, train_ratio=0.8)\nassert len(train) == 80 and len(test) == 20\nprint(f\"✓ split_data: {len(train)} train, {len(test)} test\")\n\n# Test 7: Similarity\nsim = calculate_similarity(\"hello\", \"hello\")\nassert sim == 1.0\nprint(f\"✓ calculate_similarity: 'hello' vs 'hello' = {sim:.2f}\")\n\n# Test 8: Levenshtein distance\ndist = calculate_levenshtein_distance(\"kitten\", \"sitting\")\nassert dist == 3\nprint(f\"✓ calculate_levenshtein_distance: 'kitten' vs 'sitting' = {dist}\")\n\n# Test 9: Jaccard similarity\nset1 = {'a', 'b', 'c'}\nset2 = {'b', 'c', 'd'}\njaccard = calculate_jaccard_similarity(set1, set2)\nprint(f\"✓ calculate_jaccard_similarity: {jaccard:.2f}\")\n\nprint(\"\\n✅ All utility functions tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Utilities Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package all utilities\nutils_module = {\n",
    "    'text_processing': {\n",
    "        'clean_text': clean_text,\n",
    "        'normalize_whitespace': normalize_whitespace,\n",
    "        'extract_keywords': extract_keywords,\n",
    "        'remove_duplicates': remove_duplicates,\n",
    "        'split_into_sentences': split_into_sentences,\n",
    "    },\n",
    "    'data_processing': {\n",
    "        'batch_sequences': batch_sequences,\n",
    "        'pad_sequences': pad_sequences,\n",
    "        'split_data': split_data,\n",
    "        'create_dataframe': create_dataframe,\n",
    "    },\n",
    "    'file_io': {\n",
    "        'load_json': load_json,\n",
    "        'save_json': save_json,\n",
    "        'load_pickle': load_pickle,\n",
    "        'save_pickle': save_pickle,\n",
    "        'load_text_file': load_text_file,\n",
    "        'save_text_file': save_text_file,\n",
    "    },\n",
    "    'similarity': {\n",
    "        'calculate_similarity': calculate_similarity,\n",
    "        'calculate_levenshtein_distance': calculate_levenshtein_distance,\n",
    "        'calculate_jaccard_similarity': calculate_jaccard_similarity,\n",
    "    }\n",
    "}\n",
    "\n# Save to pickle for use in other notebooks\nsave_pickle(utils_module, '/tmp/utils_module.pkl')\n\nprint(\"\\n✓ Utilities module exported to /tmp/utils_module.pkl\")\nprint(f\"✓ Total functions: {sum(len(v) for v in utils_module.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Notebook 3 Complete**\n",
    "\n",
    "### Created Functions:\n",
    "- **Text Processing**: 5 functions\n",
    "- **Data Processing**: 4 functions\n",
    "- **File I/O**: 6 functions\n",
    "- **Similarity**: 3 functions\n",
    "\n",
    "### Total: 18 utility functions\n",
    "\n",
    "**Ready for use in Notebooks 4-7**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
