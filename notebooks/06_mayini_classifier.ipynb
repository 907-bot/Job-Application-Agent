{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: MAYINI Job Classifier\n",
    "## ML-based job relevance scoring and filtering\n",
    "\n",
    "**Purpose**: Classify jobs by relevance using neural network\n",
    "\n",
    "**Can run independently?** ✅ YES (5 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy pandas scikit-learn -q\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"✓ Setup complete. Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Embedder (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobEmbedder:\n",
    "    \"\"\"\n",
    "    Convert job postings into feature vectors.\n",
    "    Creates 300-dimensional embeddings for neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Vocabulary for encoding\n",
    "        self.tech_stack = ['Python', 'Java', 'JavaScript', 'React', 'Docker', 'AWS',\n",
    "                          'Kubernetes', 'SQL', 'MongoDB', 'TensorFlow', 'PyTorch']\n",
    "        \n",
    "        self.seniority_levels = {'entry': 0, 'junior': 1, 'mid': 2, 'senior': 3, 'lead': 4}\n",
    "        self.job_types = {'full-time': 0, 'part-time': 1, 'contract': 2, 'remote': 3}\n",
    "    \n",
    "    def embed_job(self, job: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert job to embedding vector.\n",
    "        \n",
    "        Args:\n",
    "            job: Job dictionary\n",
    "        \n",
    "        Returns:\n",
    "            300-dimensional embedding\n",
    "        \"\"\"\n",
    "        embedding = []\n",
    "        \n",
    "        # 1. Tech stack encoding (50 dims)\n",
    "        tech_vector = self._encode_tech_stack(job.get('skills', []))\n",
    "        embedding.extend(tech_vector[:50])\n",
    "        \n",
    "        # 2. Seniority level encoding (10 dims)\n",
    "        seniority_vector = self._encode_seniority(job.get('seniority_level', ''))\n",
    "        embedding.extend(seniority_vector)\n",
    "        \n",
    "        # 3. Years of experience encoding (10 dims)\n",
    "        experience_vector = self._encode_experience(job.get('requirements', []))\n",
    "        embedding.extend(experience_vector)\n",
    "        \n",
    "        # 4. Job type encoding (10 dims)\n",
    "        job_type_vector = self._encode_job_type(job.get('job_type', ''))\n",
    "        embedding.extend(job_type_vector)\n",
    "        \n",
    "        # 5. Salary encoding (10 dims)\n",
    "        salary_vector = self._encode_salary(job.get('salary', ''))\n",
    "        embedding.extend(salary_vector)\n",
    "        \n",
    "        # 6. Text features (210 dims - description embedding)\n",
    "        text_vector = self._encode_text(job.get('description', ''))\n",
    "        embedding.extend(text_vector[:210])\n",
    "        \n",
    "        # Pad to 300 dimensions\n",
    "        embedding = np.array(embedding[:300])\n",
    "        if len(embedding) < 300:\n",
    "            embedding = np.pad(embedding, (0, 300 - len(embedding)), mode='constant')\n",
    "        \n",
    "        return embedding.astype(np.float32)\n",
    "    \n",
    "    def _encode_tech_stack(self, skills: List[str]) -> List[float]:\n",
    "        \"\"\"Encode technical skills as one-hot vectors\"\"\"\n",
    "        vector = [0.0] * 50\n",
    "        for i, tech in enumerate(self.tech_stack):\n",
    "            if tech.lower() in [s.lower() for s in skills]:\n",
    "                vector[i % 50] = 1.0\n",
    "        return vector\n",
    "    \n",
    "    def _encode_seniority(self, level: str) -> List[float]:\n",
    "        \"\"\"Encode seniority level\"\"\"\n",
    "        vector = [0.0] * 10\n",
    "        level_lower = level.lower()\n",
    "        for key, val in self.seniority_levels.items():\n",
    "            if key in level_lower:\n",
    "                vector[val] = 1.0\n",
    "        return vector\n",
    "    \n",
    "    def _encode_experience(self, requirements: List[str]) -> List[float]:\n",
    "        \"\"\"Encode years of experience requirement\"\"\"\n",
    "        vector = [0.0] * 10\n",
    "        text = ' '.join(requirements).lower()\n",
    "        \n",
    "        import re\n",
    "        match = re.search(r'(\\d+)\\+?\\s*(?:years?|yrs?)', text)\n",
    "        if match:\n",
    "            years = min(int(match.group(1)), 9)\n",
    "            vector[years] = 1.0\n",
    "        return vector\n",
    "    \n",
    "    def _encode_job_type(self, job_type: str) -> List[float]:\n",
    "        \"\"\"Encode job type\"\"\"\n",
    "        vector = [0.0] * 10\n",
    "        job_type_lower = job_type.lower()\n",
    "        for key, val in self.job_types.items():\n",
    "            if key in job_type_lower:\n",
    "                vector[val] = 1.0\n",
    "        return vector\n",
    "    \n",
    "    def _encode_salary(self, salary: str) -> List[float]:\n",
    "        \"\"\"Encode salary range\"\"\"\n",
    "        vector = [0.0] * 10\n",
    "        \n",
    "        import re\n",
    "        matches = re.findall(r'\\$(\\d+[,\\d]*)', salary)\n",
    "        if matches:\n",
    "            try:\n",
    "                salary_val = int(matches[0].replace(',', ''))\n",
    "                # Normalize to 0-10 scale (0-1M)\n",
    "                normalized = min(salary_val / 100000, 9)\n",
    "                vector[int(normalized)] = 1.0\n",
    "            except:\n",
    "                pass\n",
    "        return vector\n",
    "    \n",
    "    def _encode_text(self, text: str) -> List[float]:\n",
    "        \"\"\"Simple text encoding using word frequency\"\"\"\n",
    "        # Create simple TF vector\n",
    "        words = text.lower().split()\n",
    "        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'was'}\n",
    "        words = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "        \n",
    "        from collections import Counter\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        # Get hash-based features\n",
    "        vector = [0.0] * 210\n",
    "        for word, count in word_freq.most_common(210):\n",
    "            idx = hash(word) % 210\n",
    "            vector[idx] = min(count / 10.0, 1.0)\n",
    "        \n",
    "        return vector\n\nprint(\"✓ JobEmbedder class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAYINI Classifier Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAYINIRelevanceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for job relevance classification.\n",
    "    Binary classifier: relevant (1) or not relevant (0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 300, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.fc2 = nn.Linear(hidden_dim, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, 300)\n",
    "        \n",
    "        Returns:\n",
    "            Relevance score (batch_size, 1)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n\nprint(\"✓ MAYINI classifier network created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Relevance Classifier Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobRelevanceClassifier:\n",
    "    \"\"\"\n",
    "    Main classifier for job relevance.\n",
    "    Wraps embedder and neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=DEVICE):\n",
    "        self.embedder = JobEmbedder()\n",
    "        self.model = MAYINIRelevanceClassifier(input_dim=300, hidden_dim=128).to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.user_preferences = []  # Track user interactions\n",
    "    \n",
    "    def predict_relevance(self, job: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Predict relevance score for a job.\n",
    "        \n",
    "        Args:\n",
    "            job: Job dictionary\n",
    "        \n",
    "        Returns:\n",
    "            Relevance score (0-1)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Embed job\n",
    "            embedding = self.embedder.embed_job(job)\n",
    "            embedding_tensor = torch.FloatTensor(embedding).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Predict\n",
    "            score = self.model(embedding_tensor)\n",
    "            \n",
    "            return float(score.item())\n",
    "    \n",
    "    def batch_classify_jobs(self, jobs: List[Dict], threshold: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Classify multiple jobs.\n",
    "        \n",
    "        Args:\n",
    "            jobs: List of job dictionaries\n",
    "            threshold: Relevance threshold (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            Classification results\n",
    "        \"\"\"\n",
    "        relevant = []\n",
    "        irrelevant = []\n",
    "        \n",
    "        scores = []\n",
    "        for job in jobs:\n",
    "            score = self.predict_relevance(job)\n",
    "            scores.append(score)\n",
    "            \n",
    "            item = {'job': job, 'relevance_score': score}\n",
    "            \n",
    "            if score >= threshold:\n",
    "                relevant.append(item)\n",
    "            else:\n",
    "                irrelevant.append(item)\n",
    "        \n",
    "        return {\n",
    "            'relevant': sorted(relevant, key=lambda x: x['relevance_score'], reverse=True),\n",
    "            'irrelevant': sorted(irrelevant, key=lambda x: x['relevance_score'], reverse=True),\n",
    "            'statistics': {\n",
    "                'total_jobs': len(jobs),\n",
    "                'relevant_count': len(relevant),\n",
    "                'irrelevant_count': len(irrelevant),\n",
    "                'pass_rate': len(relevant) / len(jobs) if jobs else 0,\n",
    "                'avg_score': np.mean(scores),\n",
    "                'min_score': min(scores) if scores else 0,\n",
    "                'max_score': max(scores) if scores else 1,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_on_user_history(self, job_applications: List[Dict], labels: List[int], epochs: int = 10):\n",
    "        \"\"\"\n",
    "        Train classifier on user's job application history.\n",
    "        \n",
    "        Args:\n",
    "            job_applications: List of jobs user applied to\n",
    "            labels: 1 for relevant/applied, 0 for irrelevant\n",
    "            epochs: Number of training epochs\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for job, label in zip(job_applications, labels):\n",
    "                # Embed\n",
    "                embedding = self.embedder.embed_job(job)\n",
    "                embedding_tensor = torch.FloatTensor(embedding).unsqueeze(0).to(self.device)\n",
    "                label_tensor = torch.FloatTensor([label]).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Forward\n",
    "                output = self.model(embedding_tensor)\n",
    "                loss = self.criterion(output, label_tensor)\n",
    "                \n",
    "                # Backward\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(job_applications) if job_applications else 0\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n\nprint(\"✓ JobRelevanceClassifier wrapper created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample jobs\ntry:\n",
    "    with open('/tmp/job_scraper_module.pkl', 'rb') as f:\n",
    "        scraper_data = pickle.load(f)\n",
    "    sample_jobs = scraper_data['sample_jobs']\n",
    "except:\n",
    "    print(\"Creating mock jobs...\")\n",
    "    sample_jobs = [\n",
    "        {\n",
    "            'title': 'Python Developer',\n",
    "            'company': 'Tech Corp',\n",
    "            'location': 'SF',\n",
    "            'skills': ['Python', 'Docker', 'AWS'],\n",
    "            'seniority_level': 'Senior',\n",
    "            'job_type': 'Full-time',\n",
    "            'salary': '$150K - $200K',\n",
    "            'requirements': ['5+ years Python', 'Docker knowledge'],\n",
    "            'description': 'Looking for senior Python developer...'\n",
    "        }\n",
    "    ]\n",
    "\nprint(f\"✓ Loaded {len(sample_jobs)} sample jobs\")\n\n# Initialize classifier\nclassifier = JobRelevanceClassifier()\nprint(\"✓ Classifier initialized\")\n\n# Test 1: Predict single job\nprint(\"\\nTest 1: Predicting relevance...\")\nfor i, job in enumerate(sample_jobs[:3]):\n",
    "    score = classifier.predict_relevance(job)\n",
    "    print(f\"  Job {i+1} ({job['title']}): {score:.3f}\")\n\n# Test 2: Batch classification\nprint(\"\\nTest 2: Batch classification...\")\nresults = classifier.batch_classify_jobs(sample_jobs, threshold=0.5)\nprint(f\"  Total jobs: {results['statistics']['total_jobs']}\")\nprint(f\"  Relevant: {results['statistics']['relevant_count']}\")\nprint(f\"  Pass rate: {results['statistics']['pass_rate']:.1%}\")\nprint(f\"  Avg score: {results['statistics']['avg_score']:.3f}\")\n\n# Test 3: Train on user history\nprint(\"\\nTest 3: Training on user history...\")\ntraining_jobs = sample_jobs[:5]\ntraining_labels = [1, 1, 0, 1, 0]  # User's preferences\nclassifier.train_on_user_history(training_jobs, training_labels, epochs=10)\n\nprint(\"\\n✅ All classifier tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classifier\nclassifier_data = {\n",
    "    'JobRelevanceClassifier': JobRelevanceClassifier,\n",
    "    'MAYINIRelevanceClassifier': MAYINIRelevanceClassifier,\n",
    "    'JobEmbedder': JobEmbedder,\n",
    "    'classifier_instance': classifier,\n",
    "    'classification_results': results,\n",
    "}\n",
    "\nwith open('/tmp/job_classifier_module.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier_data, f)\n",
    "\nwith open('/tmp/classification_results.json', 'w') as f:\n",
    "    # Convert for JSON serialization\n",
    "    json_results = {\n",
    "        'statistics': results['statistics'],\n",
    "        'relevant_count': len(results['relevant']),\n",
    "        'irrelevant_count': len(results['irrelevant']),\n",
    "    }\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\nprint(\"✓ Job classifier exported to /tmp/job_classifier_module.pkl\")\nprint(\"✓ Results saved to /tmp/classification_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Notebook 6 Complete**\n",
    "\n",
    "### Features:\n",
    "- Job embedder (300-dimensional vectors)\n",
    "- Neural network classifier (3 hidden layers)\n",
    "- Batch classification\n",
    "- User history training\n",
    "- Relevance scoring\n",
    "\n",
    "**Ready for use in Notebook 7 (Application Agent)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
